{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch beginner course: Optimizer\n",
    "\n",
    "## Summary\n",
    "\n",
    "- [Simple training](#simple-training)\n",
    "- [Optimizer](#optimizer)\n",
    "    - [Gradient Descent Algorithm](#gradient-descent-algorithm)\n",
    "- [Optimizer in pytorch](#optimizer-in-pytorch)\n",
    "- [Glossary of the used tools](#glossary-of-the-used-tools)\n",
    "    - [Methods](#methods)\n",
    "- [References](#references)\n",
    "- [Author](#author)\n",
    "\n",
    "## Simple training\n",
    "\n",
    "Before to introduce the backpropagation concept we will implement a simple code that simulate a training scenario using only the knowledge acquired at this point of the course.\n",
    "\n",
    "To understand the concept the best way is proced step by step, so in this first version of our code we will implement only the main parts of a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor that contain our weights\n",
    "weights = torch.ones(4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Define a function to get a model with their weights\n",
    "model = (weights*3).sum()\n",
    "\n",
    "# Apply the backward on our model to compute the gradient\n",
    "model.backward()\n",
    "\n",
    "# output of the weights gradient\n",
    "print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, in output we have tensor with all values of the gradient are equals to $3$ and this means that the gradients of our weights tensor are equal to $3$.\n",
    "Now we will see what happens if we iterate the training for the model for $3 \\ epochs$ for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.], dtype=torch.float64)\n",
      "tensor([6., 6., 6., 6.], dtype=torch.float64)\n",
      "tensor([9., 9., 9., 9.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Tensor that contain our weights\n",
    "weights = torch.ones(4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "for epochs in range(3):\n",
    "    model = (weights*3).sum()\n",
    "    model.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see at each iteration the gradients calculation accumulates and we get a clearly wrong gradient coordinates, so to fix this bad accumulation we must set to zero the gradient value at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.], dtype=torch.float64)\n",
      "tensor([3., 3., 3., 3.], dtype=torch.float64)\n",
      "tensor([3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Tensor that contain our weights\n",
    "weights = torch.ones(4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "for epochs in range(3):\n",
    "    model = (weights*3).sum()\n",
    "    model.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "    # set to zero the gradient values\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the gradients computation is correctly and remember that you must pay attention to this details to avoid to get a bad model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "One of the most important tool about the machine learning algorithms is the **optimizer**, we know that the our goal consist in the *loss function minimization* and to do this we can use specific algorithms, based on the famous ***descent gradient algorithm***, we can use this kind of algorithms because our loss function is derivable.\n",
    "\n",
    "Hence, we use an optimizer algorithms to minimize the loss function, and this optimizers usually are iterative algorithm based on a *gradient descent algorithm*, the aim of the optimizers is update the parameters of our model with the loss minimization.\n",
    "\n",
    "The most famous optimizer used in machine learning are:\n",
    "\n",
    "* *Stochastic Gradient Descent (SGD)*\n",
    "* *Adaptive moment Estimation (ADAM)*\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "The gradient descent is the most important iterative algorithm to find the minima of a function, mathematically we explain the essence of the gradient descent as:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "⚠️: The quantity $\\alpha$ is so called ***learning rate*** and this value is a huge influence on the final result.\n",
    "\n",
    "As follow we can see also the ***backtracking*** approach to choose a different values for $\\alpha$ at each iteration to optimize the algorithm.\n",
    "\n",
    "At begin we set the $\\alpha_{k}$ as:\n",
    "\n",
    "$$\n",
    "\\alpha_{k} = 1\n",
    "$$\n",
    "\n",
    "Now we can compute the value of the $X$:\n",
    "\n",
    "$$\n",
    "X = x_k - \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "Now we must a check on the $X$, so if:\n",
    "\n",
    "$$\n",
    "f(X) \\leq f(x_k)-\\frac{\\alpha_k}{3} ||\\nabla f(x_k)||\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "x_{k+1} = X\n",
    "$$\n",
    "\n",
    "otherwise, we apply the backtracking and to do this we return at previous index but with decreased value for $\\alpha$ *(learning rate)*, hence:\n",
    "\n",
    "$$\n",
    "\\alpha_k = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = x_k - 0.5 \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer in pytorch\n",
    "\n",
    "Torch offer a specific module that contains all optimizer named `torch.optim`, as follow we can see a simple implementation of the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\tLoss: 4.361312389373779\n",
      "Epoch: 20\tLoss: 4.1808762550354\n",
      "Epoch: 30\tLoss: 4.007955551147461\n",
      "Epoch: 40\tLoss: 3.842235803604126\n",
      "Epoch: 50\tLoss: 3.683413028717041\n",
      "Epoch: 60\tLoss: 3.531200647354126\n",
      "Epoch: 70\tLoss: 3.3853204250335693\n",
      "Epoch: 80\tLoss: 3.24550724029541\n",
      "Epoch: 90\tLoss: 3.1115076541900635\n",
      "Epoch: 100\tLoss: 2.983078718185425\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for the pseudo random numbers\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Dataset\n",
    "x_train = torch.randn(100,1)\n",
    "y_train = 2 * x_train\n",
    "\n",
    "# Model definition\n",
    "model = torch.nn.Linear(1,1)\n",
    "\n",
    "# SGD optimizer definition with 0.001 learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Start the train\n",
    "    model.train()\n",
    "\n",
    "    # Predict the sample from the training set\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Compute the loss function on the prediction\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y_train)\n",
    "\n",
    "    # At each iteration set to zero the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Apply the backward to compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters on the new gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # print the epoch and the loss each 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1}\\tLoss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠: In the next lectures we will study better the module `torch.nn`\n",
    "\n",
    "## Glossary of the used tools\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `torch.manual_seed()`\n",
    "- `torch.optim.SGD()`\n",
    "- `torch.optim.SGD.step()`\n",
    "- `torch.optim.SGD.zero_grad()`\n",
    "- `torch.optim.SGD.step()`\n",
    "- `torch.nn.Linear()`\n",
    "\n",
    "## References\n",
    "\n",
    "[Pytorch documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "## Author\n",
    "\n",
    "Emilio Garzia, 2024\n",
    "\n",
    "[Github](https://github.com/EmilioGarzia)\n",
    "\n",
    "[Linkedin](https://www.linkedin.com/in/emilio-garzia-58a934294/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
