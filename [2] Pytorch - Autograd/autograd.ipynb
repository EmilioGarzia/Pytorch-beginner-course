{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch beginner course: Autograd (Pytorch for gradient computation)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- [What is Autograd?](#what-is-autograd)\n",
    "- [How machines compute derivates](#how-machines-compute-derivates)\n",
    "- [Gradient calculation](#gradient-calculation)\n",
    "- [Backward function and the Jacobian matrix](#backward-function-and-the-jacobian-matrix)\n",
    "- [Gradient history management](#gradient-history-management)\n",
    "- [Glossary of the used tools](#glossary-of-the-used-tools)\n",
    "    - [Methods](#methods)\n",
    "    - [Properties](#properties)\n",
    "- [References](#references)\n",
    "- [Author](#author)\n",
    "\n",
    "## What is autograd?\n",
    "\n",
    "Machine learning can be considered as an optimization problem, usually a minimization problem, for this reason the **derivates** are the main tools used by the training algorithms, thankfully pytorch offers a specifi module to solve the *derivates* and in particular the *gradients* of the functions, this module is `torch.autograd`.\n",
    "\n",
    "As we will see, the concept of the derivates is very important to understand the mechanism of a learning algorithms, and with `autograd` we will not waste time to implements manually all functions that calculate the gradients, pytorch is one of the most used deep learning framework also for the `autograd`.\n",
    "\n",
    "The main pros of the `autograd` are:\n",
    "1. *Easy to use*\n",
    "2. *Efficiency*\n",
    "3. *Flexibility*\n",
    "\n",
    "## How machines compute derivates\n",
    "\n",
    "In computer science we have three different ways to calculate derivates:\n",
    "\n",
    "* Numerical differentiation\n",
    "* Symbolic differentiation\n",
    "* Automatic differentiation *(that combine the two previous approaches)*\n",
    "\n",
    "in this lecture we will not study the theory behind this three approaches, but that's important to know the main pros and cons among them:\n",
    "\n",
    "| Method | Precision | Velocity | Application |\n",
    "|:------:|:---------:|:--------:|:-----------:|\n",
    "| Numerical Differentiation | Approximate | Rapidly | Simple function |\n",
    "| Symbolic Differentiation | Exactly | Slow | Complex function |\n",
    "| Automatic Differentiation | Exactly or Approximate | Rapidly | Complex function |\n",
    "\n",
    "The `autograd` module use the **Automatic differentiation**, which combines the **Numerical** and **Symbolic** approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient calculation\n",
    "\n",
    "Now we will see in practice how is possible to calculate the gradients with `autograd`.\n",
    "\n",
    "For the first is necessary to set the attribute `requires_grad=True` to specify that we want track the tensor during the gradients calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([4., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.,3.], requires_grad=True)\n",
    "y = torch.tensor([4.,2.], requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our tensors are ready to know their gradients during the computation.\n",
    "\n",
    "For semplicity now we apply a simple operation between this two tensors and we will save the output of this opertation into an another tensor `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24., 31.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = (x**3)+(y**2)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is very importanto to pay attention to the last output, we will see that the our tensor `z` have a particular attribute named `grad_fn` that explain the gradient function that we will use to apply the **backward** method, as we can see the gradient function is named `<AddBackward0>` and this information is very important for us because the name of gradient function explain us that the tensor `z` was born from a sum operation *(the sum between $x^3$ and $y^2$)*.\n",
    "\n",
    "The name of *gradient function* change if we change the basic operation to obtain the output tensor, as follow we show some of these functions:\n",
    "\n",
    "| Operation | Gradient Function |\n",
    "|:---------:|:-----------------:|\n",
    "| `+` | `<AddBackward>` |\n",
    "| `-` | `<SubBackward>` |\n",
    "| `*` | `<MulBackward>` |\n",
    "| `/` | `<DivBackward>` |\n",
    "| `mean()` | `<MeanBackward>` |\n",
    "\n",
    "Essentially, the `grad_fn` contain an object instance pointer of the class `torch.autograd.Function` if the tensor was made by an operation between two tensor, otherwise the `grad_fn` attribute have `None` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn of z:  <AddBackward0 object at 0x000001E41F8489A0>\n",
      "grad_fn of x:  None\n",
      "grad_fn of y:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"grad_fn of z: \", z.grad_fn)   # z was made by an operation between x and y\n",
    "print(\"grad_fn of x: \", x.grad_fn)   # x was made by a user (don't have a gradient function)\n",
    "print(\"grad_fn of y: \", y.grad_fn)   # y was made by a user (don't have a gradient function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can procede to compute a gradients of `x` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([12., 27.])\n",
      "Gradients of y: tensor([8., 4.])\n"
     ]
    }
   ],
   "source": [
    "# backward() method compute the gradients respect to the leaf of the graph (x and y are the leafs in our scenario)\n",
    "# the backward() method work only on a scalar value, for this reason we must do an operation that compress the output tensor in a scalar value,\n",
    "# in this case we have used a sum() function, but we could have used any function, for example mean()\n",
    "z = z.sum()\n",
    "\n",
    "# Now we can calculate the gradients...\n",
    "z.backward()\n",
    "\n",
    "# ...and print the gradients of our input tensor x and y\n",
    "print(\"Gradients of x:\", x.grad)\n",
    "\n",
    "print(\"Gradients of y:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output obtained is given by the calculation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z}{\\partial x}\n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3x^2\n",
    "\\\\\n",
    "2y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3x_1^2 \\quad 3x_2^2\n",
    "\\\\\n",
    "2y_1 \\quad 2y_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 \\cdot 2^2 \\quad 3 \\cdot 3^2\n",
    "\\\\\n",
    "2 \\cdot 4 \\quad 2 \\cdot 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "12 \\quad 27\n",
    "\\\\\n",
    "8 \\quad 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Below we can see the representation of our simple computation\n",
    "\n",
    "![backward concept](images/backward.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward function and the Jacobian matrix\n",
    "\n",
    "The `backward` using a ***Jacobian matrix*** to compute the gradients, in linear algebra the *Jacobian Matrix* is a particular matrix that contain all partial derivates of a function, in this matrix each column represent tha gradient respect to each variables.\n",
    "\n",
    "Letâ€™s say we have a vector of functions composed of two functions:\n",
    "\n",
    "$$\n",
    "G = \\binom{g_1}{g_2} = \\binom{y^2+a}{x^2+a}\n",
    "$$\n",
    "\n",
    "the Jacobian matrix of our $G$:\n",
    "\n",
    "$$\n",
    "J_G = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial g_1}{\\partial x_1} \\quad \\frac{\\partial g_1}{\\partial y_1}\n",
    "\\\\\n",
    "\\frac{\\partial g_2}{\\partial x_2} \\quad \\frac{\\partial g_2}{\\partial y_2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\quad 2y\n",
    "\\\\\n",
    "2x \\quad 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we specify other tensor *(with same dimension of an input tensors)* in a `backward` function we can compute the Jacobian matrix of our function multiplied by the specified vector:\n",
    "\n",
    "$$\n",
    "J \\cdot v\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4., 12.])\n",
      "tensor([27., 96.])\n"
     ]
    }
   ],
   "source": [
    "# Init two input tensors\n",
    "x = torch.tensor([2.,3.], requires_grad=True)\n",
    "y = torch.tensor([3.,4.], requires_grad=True)\n",
    "\n",
    "# compute an output tensor created by a sum operation\n",
    "z = (x**2)+(y**3)\n",
    "\n",
    "# define a tensor 1D with the same dimension of x and y\n",
    "v = torch.tensor([1.,2.], dtype=torch.float32)\n",
    "\n",
    "# dz/dx, dz/dy * v\n",
    "z.backward(v) \n",
    "\n",
    "# output of the gradients\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output can be explained by the below computation:\n",
    "\n",
    "$$\n",
    "J_z \\cdot v =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial z}{\\partial x_1} \\quad \\frac{\\partial z}{\\partial y_1}\n",
    "\\\\\n",
    "\\frac{\\partial z}{\\partial x_2} \\quad \\frac{\\partial z}{\\partial y_2}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_1\n",
    "\\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\quad 27\n",
    "\\\\\n",
    "6 \\quad 48\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1\n",
    "\\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\quad 27\n",
    "\\\\\n",
    "12 \\quad 96\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient history management\n",
    "\n",
    "Keeping track of the computational graph turns out to be very expensive in terms of computational costs, for this reason if we don't need to calculate the gradient the best way is detach the computational graph, as follow we will show you some methods and best practice to manage the gradients history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad:  True\n",
      "x requires_grad:  False\n",
      "y requires_grad:  False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "print(\"x requires_grad: \", x.requires_grad)\n",
    "\n",
    "# Set requires_grad to False\n",
    "x.requires_grad_(False)\n",
    "print(\"x requires_grad: \",x.requires_grad)    \n",
    "\n",
    "# copy the x tensor into the y tensor without the tracking of the computational graph\n",
    "y = x.detach()\n",
    "print(\"y requires_grad: \", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `with` python keyword to define a specific block of code that don't track a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor with computational graph:  tensor([3., 3., 3.], grad_fn=<AddBackward0>)\n",
      "y tensor into the 'with' code block:  tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "\n",
    "# y tensor if we track the computational graph\n",
    "y = x+2\n",
    "print(\"y tensor with computational graph: \", y)\n",
    "\n",
    "# y tensor if we don't track the computational graph\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(\"y tensor into the 'with' code block: \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of the used tools\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `torch.autograd.backward()`\n",
    "- `torch.autograd.requires_grad_()`\n",
    "- `torch.autograd.no_grad()`\n",
    "\n",
    "### Properties\n",
    "\n",
    "- `torch.autograd.grad`\n",
    "- `torch.autograd.requires_grad`\n",
    "- `torch.autograd.grad_fn`\n",
    "\n",
    "## References\n",
    "\n",
    "[Pytorch documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "## Author\n",
    "\n",
    "Emilio Garzia, 2024\n",
    "\n",
    "[Github](https://github.com/EmilioGarzia)\n",
    "\n",
    "[Linkedin](https://www.linkedin.com/in/emilio-garzia-58a934294/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
