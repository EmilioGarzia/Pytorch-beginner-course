{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch beginner course: Tensors\n",
    "\n",
    "## Summary\n",
    "\n",
    "- [What is a tensor?](#what-is-a-tensor)\n",
    "- [Initialize a tensor in Pytorch](#initialize-a-tensor-in-pytorch)\n",
    "  - [General purpose methods of pytorch](#general-purpose-methods-of-pytorch) \n",
    "- [How tensors are stored in memory](#how-tensors-are-stored-in-memory)\n",
    "- [Math operations between tensors](#math-operations-between-tensors)\n",
    "- [Main properties of a tensor](#main-properties-of-a-tensor)\n",
    "- [Glossary of the used tools](#glossary-of-the-used-tools)\n",
    "    - [Methods](#methods)\n",
    "    - [Properties](#properties)\n",
    "- [References](#references)\n",
    "- [Author](#author)\n",
    "\n",
    "## What is a tensor?\n",
    "\n",
    "In this first lecture we will see one of the most used data structure in the Machine Learning, a data structure named **tensor**.\n",
    "The tensor expands the matrix concept, in particular in the case of the matrix usually we have two dimensions, while in the tensor case we can has a variable number of dimensions, hence we can say that the tensors are a data structure that generalized the matrix *(and the array)* concept.\n",
    "\n",
    "⚠: With a tensor we can represent a vector or matrix, but with matrix with can't represent a tensor\n",
    "\n",
    "⚠: The result of a **Kronecker product** return a tensor\n",
    "\n",
    "Below we can see an image representing some form of a tensor:\n",
    "\n",
    "![Tensors](images/Tensor.png)\n",
    "\n",
    "## Initialize a tensor in Pytorch\n",
    "\n",
    "Below we can see the main methods to initialize a tensor in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPTY TENSOR\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# import the pytorch module\n",
    "import torch\n",
    "\n",
    "# Declare a tensor of 2x2 dimensions with not initialized values\n",
    "empty_tensor = torch.empty(2,2)\n",
    "\n",
    "# Print the values of the tensor\n",
    "print(\"EMPTY TENSOR\\n\", empty_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch offers many methods to initialize a tensors and below we can see some of these methods, remeber that for all init methods we can specify anything dimensions, for example in the previous scenario I could have used the `empty()` method as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPTY TENSOR WITH MANY DIMENSIONS\n",
      " tensor([[[[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "empty_tensor = torch.empty(2,2,3,2)\n",
    "\n",
    "print(\"EMPTY TENSOR WITH MANY DIMENSIONS\\n\", empty_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZEROS TENSOR\n",
      " tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "\n",
      "ONES TENSOR\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "RANDOM TENSOR\n",
      " tensor([[0.0496, 0.9556],\n",
      "        [0.5043, 0.2169]])\n",
      "\n",
      "RANDOM INTEGER TENSOR\n",
      " tensor([[7, 5],\n",
      "        [7, 8],\n",
      "        [8, 9]])\n",
      "\n",
      "CUSTOMIZED TENSOR\n",
      " tensor([[ 2, 13,  1,  4],\n",
      "        [12,  4,  3,  4]])\n"
     ]
    }
   ],
   "source": [
    "# Init some tensors\n",
    "zeros_tensor = torch.zeros(2,2,3)\n",
    "ones_tensor = torch.ones(3,3)\n",
    "rand_tensor = torch.rand(2,2)\n",
    "randint_tensor = torch.randint(low=2,high=10, size=(3,2))\n",
    "customized_tensor = torch.tensor([[2,13,1,4], [12,4,3,4]])\n",
    "\n",
    "# Output of all tensors\n",
    "print(\"ZEROS TENSOR\\n\", zeros_tensor)\n",
    "print(\"\\nONES TENSOR\\n\", ones_tensor)\n",
    "print(\"\\nRANDOM TENSOR\\n\", rand_tensor)\n",
    "print(\"\\nRANDOM INTEGER TENSOR\\n\", randint_tensor)\n",
    "print(\"\\nCUSTOMIZED TENSOR\\n\", customized_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all init methods we can specify the type of the data that the tensor contain using a `dtpye` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of my tensor:  torch.int32\n",
      "32bit INTEGER TENSOR\n",
      " tensor([[1, 1],\n",
      "        [1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "integer_tensor = torch.ones(2,2, dtype=torch.int32)  # init a tensor for integer values\n",
    "\n",
    "print(\"Type of my tensor: \", integer_tensor.dtype)   # output of the data type of my tensor\n",
    "print(\"32bit INTEGER TENSOR\\n\", integer_tensor)      # output of the my tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below i recap some data type available into the torch module:\n",
    "\n",
    "| `dtype` | Type |\n",
    "|:-------:|:----:|\n",
    "| `float32` or `float` |32bit floating point|\n",
    "| `float64` or `double` |64bit floating point|\n",
    "| `complex64` or `cfloat` |64bit complex number|\n",
    "| `int8` |8bit integer|\n",
    "| `int16` |16bit integer|\n",
    "| `int32` |32bit integer|\n",
    "| `int64` or `long`|64bit integer|\n",
    "| `uint8`|unsigned 8bit integer|\n",
    "| `bool` |boolean|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All init methods have a variant that initialize a tensor with the same dimesion as the specificated tensor in arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: torch.Size([3])\n",
      "Shape of y: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, dtype=torch.float32)      # init a simple ones tensor with Size=3\n",
    "y = torch.ones_like(x)                      # tensor y will be initialized with the same shape of x\n",
    "\n",
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How tensors are stored in memory\n",
    "\n",
    "Like the matrices *(in a classical programming language)* also the tensors are stored in a contigous space in memory, for this reason we can access to any index of our tensor using a simple equation that calculate the specific address memory index of that specific element.\n",
    "\n",
    "This simple equation use a particular value named `stride` value and we can get the value with the method `tensor.stride()`, we will have as many stride values as the dimensions of the tensor, as follow we can see this formula:\n",
    "\n",
    "$$\n",
    "target = index_1 \\cdot stride_1 + ... + index_n \\cdot stride_n\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "n = number \\ of \\ a \\ dimensions\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical access:  3\n",
      "Address access:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wy/4343q95j7nn15j3_nq_xvd3w0000gn/T/ipykernel_88438/1009503876.py:15: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  target = x.storage()[address]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "\"\"\"\n",
    "in this block of code we will catch the value 3\n",
    "\"\"\"\n",
    "\n",
    "# logical access\n",
    "target = x[1,0].item()\n",
    "print(\"Logical access: \", target)\n",
    "\n",
    "# address access\n",
    "row_index, col_index = 1, 0\n",
    "stride = x.stride()\n",
    "address = row_index*stride[0] + col_index*stride[1]\n",
    "target = x.storage()[address]\n",
    "print(\"Address access: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `storage()` is a method that return the whole raw data stored into the memory, so we can get all elements of the tensor contigously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contigously raw data\n",
      "  1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Contigously raw data\\n\", x.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General purpose methods of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is tensor:  True\n",
      "Size of x: torch.Size([4])\n",
      "Element at ([1,2]):  3\n",
      "TENSOR FROM OTHER DATA STRUCTURE: tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "\n",
    "print(\"Is tensor: \", torch.is_tensor(x))        # return true, if the obj is a tensor\n",
    "print(\"Size of x:\", x.size())                   # return the dimension of the tensor\n",
    "print(\"Element at ([1,2]): \", x[2].item())      # return the value at indicated position\n",
    "\n",
    "# Convert other data structure in a tensor\n",
    "other_data_structure = [1,2,3,4]\n",
    "converted = torch.as_tensor(other_data_structure)\n",
    "print(\"TENSOR FROM OTHER DATA STRUCTURE:\",converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is compatible with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from numpy: tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_structure = np.ones(2, dtype=int)\n",
    "tensor = torch.from_numpy(numpy_structure)\n",
    "\n",
    "print(\"Tensor from numpy:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch uses standard numpy-like indexing and slicing to access to the data of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My tensor:\n",
      " tensor([[3, 2, 4],\n",
      "        [4, 1, 5]])\n",
      "First element: 3\n",
      "First column: tensor([3, 4])\n",
      "Last column: tensor([4, 5])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.tensor([[3,2,4], [4,1,5]], dtype=torch.int64)\n",
    "\n",
    "print(f\"My tensor:\\n {my_tensor}\")\n",
    "print(f'First element: {my_tensor[0,0]}')\n",
    "print(f'First column: {my_tensor[:,0]}')\n",
    "print(f'Last column: {my_tensor[...,-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main properties of a tensor\n",
    "\n",
    "Our tensors have many properties, to follow we show some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 2, 2])\n",
      "Device: cpu\n",
      "Data type: torch.int64\n",
      "Layout:  torch.strided\n",
      "Is leaf:  True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(low=0, high=10, size=(2,2,2))\n",
    "\n",
    "print(\"Shape:\", x.shape)          # shape of tensor\n",
    "print(\"Device:\", x.device)        # device on which the tensor is stored\n",
    "print(\"Data type:\", x.dtype)      # data type of tensor\n",
    "print(\"Layout: \", x.layout)       # how the tensor is saved in memory\n",
    "print(\"Is leaf: \", x.is_leaf)     # if true, the tensor was created by the user, otherwise, the tensor was created by a previous computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math operations between tensors\n",
    "\n",
    "We can execute many mathematical operations on our tensors and we can do that using a tipical python operators, like `+`, `-`, `*`, `\\`, etc., or using the specific methods defined into the torch module *(this last practice is advised)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "\n",
      "y: tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "\n",
      "x+y: tensor([[2, 3, 4, 5],\n",
      "        [6, 7, 8, 9]])\n",
      "\n",
      "x-y: tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "\n",
      "x*y: tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "\n",
      "x/y: tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize two different tensors\n",
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "y = torch.ones(2,4, dtype=int)\n",
    "\n",
    "# Output of our original tensors\n",
    "print(\"x:\", x)\n",
    "print(\"\\ny:\", y)\n",
    "\n",
    "# Some operations\n",
    "sum = torch.add(x,y)     #equivalent to: sum = x+y\n",
    "diff = torch.sub(x,y)    #equivalent to: diff = x-y\n",
    "prod = torch.mul(x,y)    #equivalent to: prod = x*y\n",
    "div = torch.div(x,y)     #equivalent to: div = x/y\n",
    "\n",
    "# Ouptput\n",
    "print(\"\\nx+y:\", sum)\n",
    "print(\"\\nx-y:\", diff)\n",
    "print(\"\\nx*y:\", prod)\n",
    "print(\"\\nx/y:\", div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first block of code we saw the main four operations, and for each one of them we have a method to compute the operation in place:\n",
    "\n",
    "* `torch.add_()`\n",
    "* `torch.sub_()`\n",
    "* `torch.mul_()`\n",
    "* `torch.div_()`\n",
    "\n",
    "⚠: In `pytorch` many methods have \"in place\" variant that have a `_` at the end of the name of the method\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum in place:  tensor([[2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Init two ones tensors\n",
    "x = torch.ones(1,2, dtype=int)  # tensor([[1,1]])\n",
    "y = torch.ones(1,2, dtype=int)  # tensor([[1,1]])\n",
    "\n",
    "# Compute the sum in place\n",
    "x.add_(y)\n",
    "\n",
    "# Output\n",
    "print(\"Sum in place: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of course in the torch module we have many mathematiacal tools, to follow we show some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|x|: tensor([5.1500, 1.4500])\n",
      "floor(x): tensor([-6., -2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-5.15,-1.45])  # tensor with a negative values\n",
    "\n",
    "aboslute_x = torch.abs(x)  # in-place version: x.abs_()\n",
    "floor_x = torch.floor(x)   # in-place version: x.floor_()\n",
    "\n",
    "print(\"|x|:\", aboslute_x)\n",
    "print(\"floor(x):\", floor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all mathematical operations at: [Math operations pytorch documentation](https://pytorch.org/docs/stable/torch.html#math-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of the used tools\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `torch.empty()`\n",
    "- `torch.zeros()`\n",
    "- `torch.ones()`\n",
    "- `torch.rand()`\n",
    "- `torch.randint()`\n",
    "- `torch.tensor()`\n",
    "- `torch.size()`\n",
    "- `torch.add()`\n",
    "- `torch.sub()`\n",
    "- `torch.mul()`\n",
    "- `torch.div()`\n",
    "- `torch.add_()`\n",
    "- `torch.sub_()`\n",
    "- `torch.mul_()`\n",
    "- `torch.div_()`\n",
    "- `torch.abs()`\n",
    "- `torch.floor()`\n",
    "- `torch.abs_()`\n",
    "- `torch.floor_()`\n",
    "- `torch.is_tensor()`\n",
    "- `torch.as_tensor()`\n",
    "- `item()`\n",
    "- `torch.from_numpy()`\n",
    "- `torch.Tensor.stride()`\n",
    "- `torch.Tensor.storage()`\n",
    "\n",
    "### Properties\n",
    "\n",
    "- `torch.dtype`\n",
    "- `torch.shape`\n",
    "- `torch.device`\n",
    "- `torch.layout`\n",
    "- `torch.is_leaf`\n",
    "\n",
    "## References\n",
    "\n",
    "[Pytorch documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "## Author\n",
    "\n",
    "Emilio Garzia, 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
